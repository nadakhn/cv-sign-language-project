{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting httpx==0.13.3\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "     ---------------------------------------- 55.1/55.1 kB ? eta 0:00:00\n",
      "Collecting chardet==3.*\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting idna==2.*\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting rfc3986<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting httpcore==0.9.*\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 42.6/42.6 kB ? eta 0:00:00\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\advaitaa\\desktop\\term 7\\compvision\\project\\mediapipe_env\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
      "Collecting hstspreload\n",
      "  Downloading hstspreload-2024.12.1-py3-none-any.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 20.5 MB/s eta 0:00:00\n",
      "Collecting h2==3.*\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.0/65.0 kB ? eta 0:00:00\n",
      "Collecting h11<0.10,>=0.8\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 53.6/53.6 kB ? eta 0:00:00\n",
      "Collecting hpack<4,>=3.0\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting hyperframe<6,>=5.2.0\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py): started\n",
      "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
      "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17457 sha256=b37e8058bbd24585c9e9a3d707f02e3ab837ca261dff62707ff84b06f38b009a\n",
      "  Stored in directory: c:\\users\\advaitaa\\appdata\\local\\pip\\cache\\wheels\\c0\\59\\9f\\7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, sniffio, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0 sniffio-1.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from tensorflow.keras.models import load_model\n",
    "from googletrans import Translator\n",
    "from googletrans import LANGUAGES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: 'tur_N', 1: 'tur_O', 2: 'tur_P', 3: 'tur_R', 4: 'tur_S', 5: 'tur_T',\n",
    "    6: 'tur_U', 7: 'tur_V', 8: 'tur_Y', 9: 'tur_Z', 10: 'tur_D', 11: 'tur_E',\n",
    "    12: 'tur_F', 13: 'tur_G', 14: 'tur_H', 15: 'tur_I', 16: 'tur_J', 17: 'tur_K',\n",
    "    18: 'tur_L', 19: 'tur_M', 20: 'bis_Q', 21: 'bis_O', 22: 'bis_T', 23: 'bis_tur_C',\n",
    "    24: 'bis_D', 25: 'bis_U', 26: 'bis_M', 27: 'bis_K', 28: 'bis_B', 29: 'bis_Y',\n",
    "    30: 'bis_S', 31: 'bis_L', 32: 'bis_F', 33: 'bis_Z', 34: 'bis_E', 35: 'bis_G',\n",
    "    36: 'bis_P', 37: 'bis_A', 38: 'bis_X', 39: 'bis_V', 40: 'bis_R', 41: 'bis_W',\n",
    "    42: 'bis_N', 43: 'bis_I', 44: 'bis_H', 45: 'tur_A', 46: 'tur_B'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_landmarks(hand_landmarks, bbox, img_size=224):\n",
    "    # Create a completely black canvas\n",
    "    black_canvas = np.zeros((img_size, img_size, 3), dtype=np.uint8)\n",
    "\n",
    "    # Extract bounding box coordinates\n",
    "    x_min, y_min, w, h = bbox\n",
    "    max_width = w if w > 0 else 1\n",
    "    max_height = h if h > 0 else 1\n",
    "\n",
    "    for lm in hand_landmarks:\n",
    "        # Only use the 2D x, y values from the landmarks (ignoring z-depth)\n",
    "        x, y = lm[0], lm[1]\n",
    "\n",
    "        # Scale landmarks relative to bounding box\n",
    "        scaled_x = int(((x - x_min) / max_width) * img_size)\n",
    "        scaled_y = int(((y - y_min) / max_height) * img_size)\n",
    "\n",
    "        if 0 <= scaled_x < img_size and 0 <= scaled_y < img_size:\n",
    "            cv2.circle(black_canvas, (scaled_x, scaled_y), 5, (255, 255, 255), -1)\n",
    "\n",
    "    # Normalize pixel values\n",
    "    normalized_img = black_canvas / 255.0\n",
    "    return normalized_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af: afrikaans\n",
      "sq: albanian\n",
      "am: amharic\n",
      "ar: arabic\n",
      "hy: armenian\n",
      "az: azerbaijani\n",
      "eu: basque\n",
      "be: belarusian\n",
      "bn: bengali\n",
      "bs: bosnian\n",
      "bg: bulgarian\n",
      "ca: catalan\n",
      "ceb: cebuano\n",
      "ny: chichewa\n",
      "zh-cn: chinese (simplified)\n",
      "zh-tw: chinese (traditional)\n",
      "co: corsican\n",
      "hr: croatian\n",
      "cs: czech\n",
      "da: danish\n",
      "nl: dutch\n",
      "en: english\n",
      "eo: esperanto\n",
      "et: estonian\n",
      "tl: filipino\n",
      "fi: finnish\n",
      "fr: french\n",
      "fy: frisian\n",
      "gl: galician\n",
      "ka: georgian\n",
      "de: german\n",
      "el: greek\n",
      "gu: gujarati\n",
      "ht: haitian creole\n",
      "ha: hausa\n",
      "haw: hawaiian\n",
      "iw: hebrew\n",
      "he: hebrew\n",
      "hi: hindi\n",
      "hmn: hmong\n",
      "hu: hungarian\n",
      "is: icelandic\n",
      "ig: igbo\n",
      "id: indonesian\n",
      "ga: irish\n",
      "it: italian\n",
      "ja: japanese\n",
      "jw: javanese\n",
      "kn: kannada\n",
      "kk: kazakh\n",
      "km: khmer\n",
      "ko: korean\n",
      "ku: kurdish (kurmanji)\n",
      "ky: kyrgyz\n",
      "lo: lao\n",
      "la: latin\n",
      "lv: latvian\n",
      "lt: lithuanian\n",
      "lb: luxembourgish\n",
      "mk: macedonian\n",
      "mg: malagasy\n",
      "ms: malay\n",
      "ml: malayalam\n",
      "mt: maltese\n",
      "mi: maori\n",
      "mr: marathi\n",
      "mn: mongolian\n",
      "my: myanmar (burmese)\n",
      "ne: nepali\n",
      "no: norwegian\n",
      "or: odia\n",
      "ps: pashto\n",
      "fa: persian\n",
      "pl: polish\n",
      "pt: portuguese\n",
      "pa: punjabi\n",
      "ro: romanian\n",
      "ru: russian\n",
      "sm: samoan\n",
      "gd: scots gaelic\n",
      "sr: serbian\n",
      "st: sesotho\n",
      "sn: shona\n",
      "sd: sindhi\n",
      "si: sinhala\n",
      "sk: slovak\n",
      "sl: slovenian\n",
      "so: somali\n",
      "es: spanish\n",
      "su: sundanese\n",
      "sw: swahili\n",
      "sv: swedish\n",
      "tg: tajik\n",
      "ta: tamil\n",
      "te: telugu\n",
      "th: thai\n",
      "tr: turkish\n",
      "uk: ukrainian\n",
      "ur: urdu\n",
      "ug: uyghur\n",
      "uz: uzbek\n",
      "vi: vietnamese\n",
      "cy: welsh\n",
      "xh: xhosa\n",
      "yi: yiddish\n",
      "yo: yoruba\n",
      "zu: zulu\n"
     ]
    }
   ],
   "source": [
    "for lang_code, lang_name in LANGUAGES.items():\n",
    "    print(f\"{lang_code}: {lang_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_code_map = {\n",
    "    'bis': 'id',  # BISINDO maps to Indonesian ('id')\n",
    "    'turk': 'tr'  # Turkish maps to Turkish ('tr')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init translator\n",
    "translator = Translator()\n",
    "\n",
    "def translate_text(text, source_language='auto', target_language='en'):\n",
    "    translation = translator.translate(text, src=source_language, dest=target_language)\n",
    "    return translation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "28\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "26\n",
      "Input text cleared.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "34\n",
      "Cursor moved right to position: 1\n",
      "Input text cleared.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "32\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "22\n",
      "Cursor moved right to position: 1\n",
      "Cursor moved right to position: 2\n",
      "Input text cleared.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "27\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "37\n",
      "Cursor moved right to position: 2\n",
      "Cursor moved right to position: 3\n",
      "Input text cleared.\n",
      "Cursor moved left to position: 1\n",
      "Cursor moved left to position: 0\n",
      "id\n",
      "Meta\n",
      "Cursor moved right to position: 1\n",
      "Cursor moved right to position: 2\n",
      "Cursor moved right to position: 3\n",
      "Input text cleared.\n",
      "Input text cleared.\n",
      "Input text cleared.\n",
      "Input text cleared.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "26\n",
      "Input text cleared.\n",
      "Input text cleared.\n",
      "Input text cleared.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "37\n",
      "Cursor moved right to position: 1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "33\n",
      "Input text cleared.\n",
      "Cursor moved right to position: 1\n",
      "Cursor moved right to position: 2\n",
      "Input text cleared.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "23\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "22\n",
      "Cursor moved right to position: 2\n",
      "Input text cleared.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "33\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "37\n",
      "Cursor moved right to position: 2\n",
      "Cursor moved right to position: 3\n",
      "Input text cleared.\n",
      "id\n",
      "EYE\n",
      "Cursor moved left to position: 1\n",
      "Cursor moved left to position: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Initialize video capture and hand detector\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=2)\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('doubleCNN_landmarks.h5') # TODO: change model here\n",
    "\n",
    "# Initialize parameters\n",
    "offset = 45\n",
    "\n",
    "recognized_labels = []  #list of alphabets\n",
    "input_text = [] #input to the translator; this is what the user signs in written word\n",
    "output_text = \"\"\n",
    "last_capture_time = 0\n",
    "pause_time = 3  # pause in between frames; NOTE: can remove if u want!\n",
    "img_size = 224  # resize target for preprocessing\n",
    "language_counts = {\n",
    "    'bis': 0,\n",
    "    'tur': 0\n",
    "}\n",
    "cursor_index = 0\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    current_time = time.time()-5\n",
    "\n",
    "    # Detect hands in the frame\n",
    "    hands, img = detector.findHands(img, draw=True)\n",
    "\n",
    "    #ad's code\n",
    "    if hands and (current_time - last_capture_time > pause_time):\n",
    "        # Initialize variables for the combined bounding box\n",
    "        x_min, y_min = float('inf'), float('inf')\n",
    "        x_max, y_max = float('-inf'), float('-inf')\n",
    "\n",
    "        # Loop through detected hands to compute the combined bounding box\n",
    "        for hand in hands:\n",
    "            x, y, w, h = hand['bbox']\n",
    "            x_min = min(x_min, x - offset)\n",
    "            y_min = min(y_min, y - offset)\n",
    "            x_max = max(x_max, x + w + offset)\n",
    "            y_max = max(y_max, y + h + offset)\n",
    "\n",
    "        # Ensure the bounding box is within image boundaries\n",
    "        x_min = max(0, x_min)\n",
    "        y_min = max(0, y_min)\n",
    "        x_max = min(img.shape[1], x_max)\n",
    "        y_max = min(img.shape[0], y_max)\n",
    "\n",
    "        # Draw the single combined bounding box\n",
    "        cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (255, 0, 255), 2)\n",
    "\n",
    "        img_just_hands = img[int(y_min):int(y_max), int(x_min):int(x_max)].copy()\n",
    "        # Black background for landmarks only\n",
    "        img_landmarks = np.zeros_like(img)\n",
    "        # Draw landmarks for both hands\n",
    "        for hand in hands:\n",
    "            lm_list = hand['lmList']  # List of hand landmarks\n",
    "            for lm in lm_list:\n",
    "                cx, cy = lm[:2]\n",
    "                cv2.circle(img, (cx, cy), 5, (0, 255, 0), cv2.FILLED)\n",
    "                cv2.circle(img_landmarks, (cx, cy), 5, (0, 255, 0), cv2.FILLED)\n",
    "\n",
    "        # Crop the image based on the combined bounding box\n",
    "        img_crop = img[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "        img_landmarks_crop = img_landmarks[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "\n",
    "        # Resize and pad the cropped image to maintain aspect ratio\n",
    "        aspect_ratio = (y_max - y_min) / (x_max - x_min)\n",
    "        img_pad = np.zeros((img_size, img_size, 3), np.uint8)\n",
    "        img_landmarks_pad = np.zeros((img_size, img_size, 3), np.uint8)\n",
    "\n",
    "        if aspect_ratio > 1:\n",
    "            # Height is greater; resize width\n",
    "            k = img_size / (y_max - y_min)\n",
    "            new_w = math.ceil(k * (x_max - x_min))\n",
    "            img_resize = cv2.resize(img_crop, (new_w, img_size))\n",
    "            img_landmarks_resize = cv2.resize(img_landmarks_crop, (new_w, img_size))\n",
    "            w_offset = (img_size - new_w) // 2\n",
    "            img_pad[:, w_offset:w_offset + new_w] = img_resize\n",
    "            img_landmarks_pad[:, w_offset:w_offset + new_w] = img_landmarks_resize\n",
    "        else:\n",
    "            # Width is greater; resize height\n",
    "            k = img_size / (x_max - x_min)\n",
    "            new_h = math.ceil(k * (y_max - y_min))\n",
    "            img_resize = cv2.resize(img_crop, (img_size, new_h))\n",
    "            img_landmarks_resize = cv2.resize(img_landmarks_crop, (img_size, new_h))\n",
    "            h_offset = (img_size - new_h) // 2\n",
    "            img_pad[h_offset:h_offset + new_h, :] = img_resize\n",
    "            img_landmarks_pad[h_offset:h_offset + new_h, :] = img_landmarks_resize\n",
    "\n",
    "        img_landmarks_normalized = img_landmarks_pad / 255.0\n",
    "        img_input = np.expand_dims(img_landmarks_normalized, axis=0)  # shape: (1, 224, 224, 3)\n",
    "\n",
    "        #predicting the letter\n",
    "        try:\n",
    "            prediction = model.predict(img_input)\n",
    "            predicted_index = np.argmax(prediction[0])\n",
    "            print(predicted_index)\n",
    "            predicted_label = label_map.get(predicted_index)\n",
    "\n",
    "            if predicted_label != \"Unknown\":\n",
    "                recognized_labels.append(predicted_label)\n",
    "                input_text.append(predicted_label.split('_')[-1])  # Add only new letter\n",
    "\n",
    "            # Update language counts\n",
    "            language = predicted_label.split('_')[0]\n",
    "            language_counts[language] += 1\n",
    "\n",
    "            last_capture_time = current_time  # Reset timer\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    # Display the recognized sequence in the top-left corner\n",
    "    if input_text:  # Display only if there's input\n",
    "        # cv2.putText(img, 'Input:', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 255, 0), 3)\n",
    "        # cv2.putText(img, ''.join(input_text), (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 4)\n",
    "        display_text = ''.join(input_text)\n",
    "        # Highlight the current cursor position\n",
    "        if 0 <= cursor_index < len(input_text):\n",
    "            highlighted_text = (\n",
    "                display_text[:cursor_index] +\n",
    "                \"[\" + display_text[cursor_index] + \"]\" +\n",
    "                display_text[cursor_index + 1:]\n",
    "            )\n",
    "        else:\n",
    "            highlighted_text = display_text\n",
    "        cv2.putText(img, highlighted_text, (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 4)\n",
    "\n",
    "    # Display the translated output text in the bottom-right corner\n",
    "    if output_text:\n",
    "        output_header = \"Output:\"\n",
    "        text_size_header = cv2.getTextSize(output_header, cv2.FONT_HERSHEY_SIMPLEX, 1.4, 3)[0]\n",
    "        text_size_output = cv2.getTextSize(output_text, cv2.FONT_HERSHEY_SIMPLEX, 2, 4)[0]\n",
    "\n",
    "        x_header = img.shape[1] - text_size_header[0] - 20\n",
    "        y_header = img.shape[0] - text_size_output[1] - 20\n",
    "\n",
    "        x_output = img.shape[1] - text_size_output[0] - 20\n",
    "        y_output = img.shape[0] - 20\n",
    "\n",
    "        # Draw output header and text\n",
    "        cv2.putText(img, output_header, (x_header, y_header - 20), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 255, 0), 3)  # Double size\n",
    "        cv2.putText(img, output_text, (x_output, y_output), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 4)  # Double size\n",
    "\n",
    "    # Display instructions in the bottom-left corner\n",
    "    instructions = [\n",
    "        \"Press 'D' to delete input\",\n",
    "        \"Press 'T' to translate\",\n",
    "        \"Press 'Q' to quit\"\n",
    "    ]\n",
    "\n",
    "    y_start = img.shape[0] - 80\n",
    "    line_height = 30\n",
    "\n",
    "    for i, text in enumerate(instructions):\n",
    "        y_position = y_start + i * line_height\n",
    "        cv2.putText(img, text, (10, y_position), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "\n",
    "    # Display the live video feed\n",
    "    cv2.imshow(\"Image\", img)\n",
    "\n",
    "    #key press\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('t'):  # Translate\n",
    "        if input_text:\n",
    "            input_text_str = ''.join(input_text)\n",
    "            input_language = max(language_counts, key=language_counts.get) # bis or turk\n",
    "            input_language_code = language_code_map.get(input_language, 'auto')  # maps to google language code, default to 'auto' if not found\n",
    "            print(input_language_code)\n",
    "            output_text = translate_text(input_text_str, source_language=input_language_code, target_language='en') #TODO: create input to change the target language\n",
    "\n",
    "            print(output_text)\n",
    "\n",
    "    elif key == ord('d'):  # Delete\n",
    "        if (input_text):\n",
    "            #input_text.clear()\n",
    "            #input_text.pop(0)\n",
    "            deleted_char = input_text[cursor_index]\n",
    "            for language, count in language_counts.items():\n",
    "                if f\"{language}_{deleted_char}\" in recognized_labels:\n",
    "                    language_counts[language] = max(0, count - 1)\n",
    "            input_text.pop(cursor_index)\n",
    "            cursor_index = max(0, cursor_index - 1)\n",
    "        if (output_text):\n",
    "            output_text = \"\"\n",
    "        print(\"Input text cleared.\")\n",
    "\n",
    "    elif key == ord('a'):  # Left arrow key\n",
    "        if cursor_index > 0:\n",
    "            cursor_index -= 1\n",
    "        print(f\"Cursor moved left to position: {cursor_index}\")\n",
    "\n",
    "    elif key == ord('s'):  # Right arrow key\n",
    "        if cursor_index < len(input_text) - 1:\n",
    "            cursor_index += 1\n",
    "        print(f\"Cursor moved right to position: {cursor_index}\")\n",
    "\n",
    "    elif key == ord('q'):  # Quit\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
