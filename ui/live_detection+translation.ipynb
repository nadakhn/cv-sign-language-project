{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googletrans==4.0.0-rc1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: httpx==0.13.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
      "Requirement already satisfied: hstspreload in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.12.1)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
      "Requirement already satisfied: chardet==3.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from tensorflow.keras.models import load_model\n",
    "from googletrans import Translator\n",
    "from googletrans import LANGUAGES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: 'tur_N', 1: 'tur_O', 2: 'tur_P', 3: 'tur_R', 4: 'tur_S', 5: 'tur_T', \n",
    "    6: 'tur_U', 7: 'tur_V', 8: 'tur_Y', 9: 'tur_Z', 10: 'tur_D', 11: 'tur_E', \n",
    "    12: 'tur_F', 13: 'tur_G', 14: 'tur_H', 15: 'tur_I', 16: 'tur_J', 17: 'tur_K', \n",
    "    18: 'tur_L', 19: 'tur_M', 20: 'bis_Q', 21: 'bis_O', 22: 'bis_T', 23: 'bis_tur_C', \n",
    "    24: 'bis_D', 25: 'bis_U', 26: 'bis_M', 27: 'bis_K', 28: 'bis_B', 29: 'bis_Y', \n",
    "    30: 'bis_S', 31: 'bis_L', 32: 'bis_F', 33: 'bis_Z', 34: 'bis_E', 35: 'bis_G', \n",
    "    36: 'bis_P', 37: 'bis_A', 38: 'bis_X', 39: 'bis_V', 40: 'bis_R', 41: 'bis_W', \n",
    "    42: 'bis_N', 43: 'bis_I', 44: 'bis_H', 45: 'tur_A', 46: 'tur_B'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_landmarks(hand_landmarks, bbox, img_size=224):\n",
    "    # Create a completely black canvas\n",
    "    black_canvas = np.zeros((img_size, img_size, 3), dtype=np.uint8)\n",
    "\n",
    "    # Extract bounding box coordinates\n",
    "    x_min, y_min, w, h = bbox\n",
    "    max_width = w if w > 0 else 1\n",
    "    max_height = h if h > 0 else 1\n",
    "\n",
    "    for lm in hand_landmarks:\n",
    "        # Only use the 2D x, y values from the landmarks (ignoring z-depth)\n",
    "        x, y = lm[0], lm[1]\n",
    "        \n",
    "        # Scale landmarks relative to bounding box\n",
    "        scaled_x = int(((x - x_min) / max_width) * img_size)\n",
    "        scaled_y = int(((y - y_min) / max_height) * img_size)\n",
    "        \n",
    "        if 0 <= scaled_x < img_size and 0 <= scaled_y < img_size:\n",
    "            cv2.circle(black_canvas, (scaled_x, scaled_y), 5, (255, 255, 255), -1)\n",
    "\n",
    "    # Normalize pixel values\n",
    "    normalized_img = black_canvas / 255.0\n",
    "    return normalized_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af: afrikaans\n",
      "sq: albanian\n",
      "am: amharic\n",
      "ar: arabic\n",
      "hy: armenian\n",
      "az: azerbaijani\n",
      "eu: basque\n",
      "be: belarusian\n",
      "bn: bengali\n",
      "bs: bosnian\n",
      "bg: bulgarian\n",
      "ca: catalan\n",
      "ceb: cebuano\n",
      "ny: chichewa\n",
      "zh-cn: chinese (simplified)\n",
      "zh-tw: chinese (traditional)\n",
      "co: corsican\n",
      "hr: croatian\n",
      "cs: czech\n",
      "da: danish\n",
      "nl: dutch\n",
      "en: english\n",
      "eo: esperanto\n",
      "et: estonian\n",
      "tl: filipino\n",
      "fi: finnish\n",
      "fr: french\n",
      "fy: frisian\n",
      "gl: galician\n",
      "ka: georgian\n",
      "de: german\n",
      "el: greek\n",
      "gu: gujarati\n",
      "ht: haitian creole\n",
      "ha: hausa\n",
      "haw: hawaiian\n",
      "iw: hebrew\n",
      "he: hebrew\n",
      "hi: hindi\n",
      "hmn: hmong\n",
      "hu: hungarian\n",
      "is: icelandic\n",
      "ig: igbo\n",
      "id: indonesian\n",
      "ga: irish\n",
      "it: italian\n",
      "ja: japanese\n",
      "jw: javanese\n",
      "kn: kannada\n",
      "kk: kazakh\n",
      "km: khmer\n",
      "ko: korean\n",
      "ku: kurdish (kurmanji)\n",
      "ky: kyrgyz\n",
      "lo: lao\n",
      "la: latin\n",
      "lv: latvian\n",
      "lt: lithuanian\n",
      "lb: luxembourgish\n",
      "mk: macedonian\n",
      "mg: malagasy\n",
      "ms: malay\n",
      "ml: malayalam\n",
      "mt: maltese\n",
      "mi: maori\n",
      "mr: marathi\n",
      "mn: mongolian\n",
      "my: myanmar (burmese)\n",
      "ne: nepali\n",
      "no: norwegian\n",
      "or: odia\n",
      "ps: pashto\n",
      "fa: persian\n",
      "pl: polish\n",
      "pt: portuguese\n",
      "pa: punjabi\n",
      "ro: romanian\n",
      "ru: russian\n",
      "sm: samoan\n",
      "gd: scots gaelic\n",
      "sr: serbian\n",
      "st: sesotho\n",
      "sn: shona\n",
      "sd: sindhi\n",
      "si: sinhala\n",
      "sk: slovak\n",
      "sl: slovenian\n",
      "so: somali\n",
      "es: spanish\n",
      "su: sundanese\n",
      "sw: swahili\n",
      "sv: swedish\n",
      "tg: tajik\n",
      "ta: tamil\n",
      "te: telugu\n",
      "th: thai\n",
      "tr: turkish\n",
      "uk: ukrainian\n",
      "ur: urdu\n",
      "ug: uyghur\n",
      "uz: uzbek\n",
      "vi: vietnamese\n",
      "cy: welsh\n",
      "xh: xhosa\n",
      "yi: yiddish\n",
      "yo: yoruba\n",
      "zu: zulu\n"
     ]
    }
   ],
   "source": [
    "for lang_code, lang_name in LANGUAGES.items():\n",
    "    print(f\"{lang_code}: {lang_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_code_map = {\n",
    "    'bis': 'id',  # BISINDO maps to Indonesian ('id')\n",
    "    'turk': 'tr'  # Turkish maps to Turkish ('tr')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init translator\n",
    "translator = Translator()\n",
    "\n",
    "def translate_text(text, source_language='auto', target_language='en'):\n",
    "    translation = translator.translate(text, src=source_language, dest=target_language)\n",
    "    return translation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734019925.618584 6818570 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "W0000 00:00:1734019925.679558 6824305 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1734019925.696139 6824305 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step\n",
      "34\n",
      "Input text cleared.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "41\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "37\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "1\n",
      "id\n",
      "Wao\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "21\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "21\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Initialize video capture and hand detector\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=2)\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('../models/slt_model_rev2.h5') # TODO: change model here\n",
    "\n",
    "# Initialize parameters\n",
    "offset = 45\n",
    "\n",
    "recognized_labels = []  #list of alphabets\n",
    "input_text = [] #input to the translator; this is what the user signs in written word\n",
    "output_text = \"\"\n",
    "last_capture_time = 0\n",
    "pause_time = 5  # pause in between frames; NOTE: can remove if u want!\n",
    "img_size = 224  # resize target for preprocessing\n",
    "language_counts = {\n",
    "    'bis': 0,\n",
    "    'tur': 0\n",
    "}\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    current_time = time.time()-5\n",
    "\n",
    "    # Detect hands in the frame\n",
    "    hands, img = detector.findHands(img, draw=True) \n",
    "\n",
    "    #ad's code\n",
    "    if hands and (current_time - last_capture_time > pause_time):\n",
    "        # Initialize variables for the combined bounding box\n",
    "        x_min, y_min = float('inf'), float('inf')\n",
    "        x_max, y_max = float('-inf'), float('-inf')\n",
    "\n",
    "        # Loop through detected hands to compute the combined bounding box\n",
    "        for hand in hands:\n",
    "            x, y, w, h = hand['bbox']\n",
    "            x_min = min(x_min, x - offset)\n",
    "            y_min = min(y_min, y - offset)\n",
    "            x_max = max(x_max, x + w + offset)\n",
    "            y_max = max(y_max, y + h + offset)\n",
    "\n",
    "        # Ensure the bounding box is within image boundaries\n",
    "        x_min = max(0, x_min)\n",
    "        y_min = max(0, y_min)\n",
    "        x_max = min(img.shape[1], x_max)\n",
    "        y_max = min(img.shape[0], y_max)\n",
    "\n",
    "        # Draw the single combined bounding box\n",
    "        cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (255, 0, 255), 2)\n",
    "\n",
    "        img_just_hands = img[int(y_min):int(y_max), int(x_min):int(x_max)].copy()\n",
    "        # Black background for landmarks only\n",
    "        img_landmarks = np.zeros_like(img)\n",
    "        # Draw landmarks for both hands\n",
    "        for hand in hands:\n",
    "            lm_list = hand['lmList']  # List of hand landmarks\n",
    "            for lm in lm_list:\n",
    "                cx, cy = lm[:2]\n",
    "                cv2.circle(img, (cx, cy), 5, (0, 255, 0), cv2.FILLED)\n",
    "                cv2.circle(img_landmarks, (cx, cy), 5, (0, 255, 0), cv2.FILLED)\n",
    "\n",
    "        # Crop the image based on the combined bounding box\n",
    "        img_crop = img[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "        img_landmarks_crop = img_landmarks[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "\n",
    "        # Resize and pad the cropped image to maintain aspect ratio\n",
    "        aspect_ratio = (y_max - y_min) / (x_max - x_min)\n",
    "        img_pad = np.zeros((img_size, img_size, 3), np.uint8)\n",
    "        img_landmarks_pad = np.zeros((img_size, img_size, 3), np.uint8)\n",
    "\n",
    "        if aspect_ratio > 1:\n",
    "            # Height is greater; resize width\n",
    "            k = img_size / (y_max - y_min)\n",
    "            new_w = math.ceil(k * (x_max - x_min))\n",
    "            img_resize = cv2.resize(img_crop, (new_w, img_size))\n",
    "            img_landmarks_resize = cv2.resize(img_landmarks_crop, (new_w, img_size))\n",
    "            w_offset = (img_size - new_w) // 2\n",
    "            img_pad[:, w_offset:w_offset + new_w] = img_resize\n",
    "            img_landmarks_pad[:, w_offset:w_offset + new_w] = img_landmarks_resize\n",
    "        else:\n",
    "            # Width is greater; resize height\n",
    "            k = img_size / (x_max - x_min)\n",
    "            new_h = math.ceil(k * (y_max - y_min))\n",
    "            img_resize = cv2.resize(img_crop, (img_size, new_h))\n",
    "            img_landmarks_resize = cv2.resize(img_landmarks_crop, (img_size, new_h))\n",
    "            h_offset = (img_size - new_h) // 2\n",
    "            img_pad[h_offset:h_offset + new_h, :] = img_resize\n",
    "            img_landmarks_pad[h_offset:h_offset + new_h, :] = img_landmarks_resize\n",
    "    \n",
    "        img_landmarks_normalized = img_landmarks_pad / 255.0\n",
    "        img_input = np.expand_dims(img_landmarks_normalized, axis=0)  # shape: (1, 224, 224, 3)\n",
    "\n",
    "        #predicting the letter\n",
    "        try:\n",
    "            prediction = model.predict(img_input)\n",
    "            predicted_index = np.argmax(prediction[0])\n",
    "            print(predicted_index)\n",
    "            predicted_label = label_map.get(predicted_index)\n",
    "\n",
    "            if predicted_label != \"Unknown\":\n",
    "                recognized_labels.append(predicted_label)\n",
    "                input_text.append(predicted_label.split('_')[-1])  # Add only new letter\n",
    "\n",
    "            # Update language counts\n",
    "            language = predicted_label.split('_')[0]\n",
    "            language_counts[language] += 1\n",
    "\n",
    "            last_capture_time = current_time  # Reset timer\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    # Display the recognized sequence in the top-left corner\n",
    "    if input_text:  # Display only if there's input\n",
    "        cv2.putText(img, 'Input:', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 255, 0), 3)\n",
    "        cv2.putText(img, ''.join(input_text), (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 4)\n",
    "\n",
    "    # Display the translated output text in the bottom-right corner\n",
    "    if output_text:\n",
    "        output_header = \"Output:\"\n",
    "        text_size_header = cv2.getTextSize(output_header, cv2.FONT_HERSHEY_SIMPLEX, 1.4, 3)[0]\n",
    "        text_size_output = cv2.getTextSize(output_text, cv2.FONT_HERSHEY_SIMPLEX, 2, 4)[0]\n",
    "\n",
    "        x_header = img.shape[1] - text_size_header[0] - 20\n",
    "        y_header = img.shape[0] - text_size_output[1] - 20\n",
    "\n",
    "        x_output = img.shape[1] - text_size_output[0] - 20\n",
    "        y_output = img.shape[0] - 20\n",
    "\n",
    "        # Draw output header and text\n",
    "        cv2.putText(img, output_header, (x_header, y_header - 20), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 255, 0), 3)  # Double size\n",
    "        cv2.putText(img, output_text, (x_output, y_output), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 4)  # Double size\n",
    "    \n",
    "    # Display instructions in the bottom-left corner\n",
    "    instructions = [\n",
    "        \"Press 'D' to delete input\",\n",
    "        \"Press 'T' to translate\",\n",
    "        \"Press 'Q' to quit\"\n",
    "    ]\n",
    "\n",
    "    y_start = img.shape[0] - 80\n",
    "    line_height = 30\n",
    "\n",
    "    for i, text in enumerate(instructions):\n",
    "        y_position = y_start + i * line_height\n",
    "        cv2.putText(img, text, (10, y_position), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "\n",
    "    # Display the live video feed\n",
    "    cv2.imshow(\"Image\", img)\n",
    "\n",
    "    #key press\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('t'):  # Translate\n",
    "        if input_text:\n",
    "            input_text_str = ''.join(input_text)\n",
    "            input_language = max(language_counts, key=language_counts.get) # bis or turk\n",
    "            input_language_code = language_code_map.get(input_language, 'auto')  # maps to google language code, default to 'auto' if not found\n",
    "            print(input_language_code)\n",
    "            output_text = translate_text(input_text_str, source_language=input_language_code, target_language='en') #TODO: create input to change the target language\n",
    "\n",
    "            print(output_text)\n",
    "\n",
    "    elif key == ord('d'):  # Delete\n",
    "        if (input_text):\n",
    "            input_text.clear() \n",
    "        if (output_text):\n",
    "            output_text = \"\"\n",
    "        print(\"Input text cleared.\")\n",
    "\n",
    "    elif key == ord('q'):  # Quit\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
